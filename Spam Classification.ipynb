{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yc1PgqamHXT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "e3FmUxTnmXqK",
        "outputId": "00dd8551-c1b6-4869-c5d1-ddba8d157e36"
      },
      "source": [
        "df = pd.read_csv(\"spamdata_v2.csv\")\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0  Go until jurong point, crazy.. Available only ...\n",
              "1      0                      Ok lar... Joking wif u oni...\n",
              "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      0  U dun say so early hor... U c already then say...\n",
              "4      0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2-g_TpxmhbS"
      },
      "source": [
        "x_trainval, x_test,y_trainval,y_test = train_test_split(df['text'], df['label'],random_state=2020, test_size=0.2, stratify=df['label'])\n",
        "\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_trainval,y_trainval, random_state=2020, test_size=0.25, stratify=y_trainval)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P72M-vvw3knt",
        "outputId": "e4534994-68fd-484d-85ef-7c841676677f"
      },
      "source": [
        "bert = AutoModel.from_pretrained('bert-base-uncased');\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased');\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True)\n",
        "print(sent_id)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "OhssaooG4VIv",
        "outputId": "a9fa1f77-1dfa-4fea-fdc3-ca280b78f906"
      },
      "source": [
        "sentence_length = [len(i.split()) for i in x_trainval]\n",
        "pd.Series(sentence_length).hist(bins = 30)\n",
        "pd.Series(sentence_length).describe()\n",
        "\n",
        "#75% sentence have length of 23.Hence assigning max=length=23"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    4457.000000\n",
              "mean       15.565851\n",
              "std        11.327951\n",
              "min         1.000000\n",
              "25%         7.000000\n",
              "50%        12.000000\n",
              "75%        23.000000\n",
              "max       162.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pknoE9YV5Qr3"
      },
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(x_train.tolist(),max_length = 23,padding='max_length',truncation=True)\n",
        "tokens_val   = tokenizer.batch_encode_plus(x_val.tolist()  ,max_length = 23,padding='max_length',truncation=True)\n",
        "tokens_test  = tokenizer.batch_encode_plus(x_test.tolist() ,max_length = 23,padding='max_length',truncation=True)\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(y_train.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(y_val.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(y_test.tolist())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1qLswiG6NnK"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train)\n",
        "train_dataloader = DataLoader(train, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val)\n",
        "val_dataloader = DataLoader(val, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJuI4v-Y7AnX"
      },
      "source": [
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "class BERT_custom(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_custom, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.relu =  nn.ReLU()\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b93ha31r7bWy",
        "outputId": "6f4dea00-ef59-426e-8f0a-02878857d374"
      },
      "source": [
        "model = BERT_custom(bert)\n",
        "model = model.to(device)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT_custom(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftr_UJIa8mux",
        "outputId": "cca49efc-011f-4849-fc17-6844250f61a6"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),lr = 1e-5) \n",
        "cross_entropy  = nn.NLLLoss()\n",
        "epochs = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "valid_accuracy=[]\n",
        "tk=tqdm(range(epochs), total = epochs,desc=\"Epochs\", position = 0, leave = True)\n",
        "for epoch in tk:\n",
        "\n",
        "    # print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "    # print('-' * 10)\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    counter = 0\n",
        "    \n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        sent_id, mask, labels = batch\n",
        "        \n",
        "        model.zero_grad()\n",
        "        outputs = model(sent_id, mask)\n",
        "        loss = cross_entropy(outputs, labels)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "\n",
        "    mean_train_loss = np.mean(train_loss)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss=[]\n",
        "    val_accuracy=[]\n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "\n",
        "      # print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      sent_id, mask, labels = batch\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        outputs = model(sent_id, mask)\n",
        "\n",
        "        loss = cross_entropy(outputs,labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        val_accuracy.append(np.mean(np.equal(np.argmax(outputs.detach().cpu().numpy(),axis=1),labels.detach().cpu().numpy())))\n",
        "\n",
        "    mean_valid_loss = np.mean(val_loss)\n",
        "    mean_valid_acc  = np.mean(val_accuracy)\n",
        "\n",
        "    tk.set_postfix({'Training Loss': mean_train_loss,'Validation Loss': mean_valid_loss,'Validation Accuracy': mean_valid_acc*100})\n",
        "    tk.update()\n",
        "\n",
        "    #save the best model\n",
        "    if mean_valid_loss < best_valid_loss:\n",
        "        best_valid_loss = mean_valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(mean_train_loss)\n",
        "    valid_losses.append(mean_valid_loss)\n",
        "    valid_accuracy.append(mean_valid_acc)\n",
        "\n",
        "    \n",
        "print(f'\\nTraining Loss: {np.mean(train_losses):.3f}')\n",
        "print(f'Validation Loss: {np.mean(valid_losses):.3f}')\n",
        "print(f'Validation Accuracy: {np.mean(valid_accuracy):.3f}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 10/10 [01:16<00:00,  7.67s/it, Training Loss=0.172, Validation Loss=0.173, Validation Accuracy=0.94]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Loss: 0.182\n",
            "Validation Loss: 0.182\n",
            "Validation Accuracy: 0.935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMFByX4bVBGc",
        "outputId": "0087e561-730b-4031-c159-2db5b7b6f388"
      },
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.99      0.97       966\n",
            "           1       0.94      0.64      0.76       149\n",
            "\n",
            "    accuracy                           0.95      1115\n",
            "   macro avg       0.94      0.82      0.87      1115\n",
            "weighted avg       0.95      0.95      0.94      1115\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}